{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This site contains a collection of posts on different causal inference topics.</p> <ul> <li>Uplift modelling<ul> <li>Intro to uplift modelling</li> <li>Intro to causal forest</li> </ul> </li> </ul>"},{"location":"notebooks/causal_forest/","title":"Causal forest","text":"In\u00a0[29]: Copied! <pre>from IPython.display import Image\nImage('diagrams/causal_forest/causal_forest.md.1.png')\n</pre> from IPython.display import Image Image('diagrams/causal_forest/causal_forest.md.1.png') Out[29]: In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nnum_covariates = 10\nnum_samples = 100000\nnp.random.seed(42)\n\ndf = pd.DataFrame()\nfor i in range(num_covariates):\n  df[f'x{i}'] = np.random.randn(num_samples)  \n  df[f'y_x{i}'] = df[f'x{i}'] * np.random.uniform(low=-3, high=3) + np.random.randn(1)\n\ndf['TE'] = df['x1'].apply(lambda x: 8 if x&gt;0.1 else 0)\ndf['propensity'] = df['x2'].apply(lambda x: 0.8 if -0.5&lt;x&lt;0.5 else 0.2)\ndf['T'] = df['propensity'].apply(lambda x: np.random.binomial(n=1, p=x))\ndf['Y'] = df['TE'] * df['T'] + df[[col for col in df.columns if 'y_x' in col]].sum(axis=1)\n\ndf[[col for col in df.columns if 'y_x' not in col]].describe().transpose().drop(columns='count').drop('propensity')\n</pre> import numpy as np import pandas as pd  num_covariates = 10 num_samples = 100000 np.random.seed(42)  df = pd.DataFrame() for i in range(num_covariates):   df[f'x{i}'] = np.random.randn(num_samples)     df[f'y_x{i}'] = df[f'x{i}'] * np.random.uniform(low=-3, high=3) + np.random.randn(1)  df['TE'] = df['x1'].apply(lambda x: 8 if x&gt;0.1 else 0) df['propensity'] = df['x2'].apply(lambda x: 0.8 if -0.5 Out[2]: mean std min 25% 50% 75% max x0 0.000967 1.000906 -4.465604 -0.674494 0.002650 0.676915 4.479084 x1 0.003090 1.000497 -4.080833 -0.667894 0.000411 0.672561 4.694473 x2 -0.001496 0.999855 -4.413886 -0.674197 -0.002710 0.672063 4.219366 x3 0.001442 1.002386 -4.301410 -0.675981 0.005557 0.676334 4.293276 x4 -0.006266 1.001158 -4.829436 -0.684352 -0.008103 0.672384 4.301848 x5 -0.001142 1.000885 -4.111276 -0.679754 -0.004396 0.674232 4.295946 x6 0.003354 0.998648 -4.319465 -0.672365 0.008510 0.679251 4.526784 x7 -0.001657 0.998657 -4.386303 -0.676386 0.001485 0.673570 4.329187 x8 -0.003169 1.000302 -3.928834 -0.678930 -0.005792 0.673594 3.980280 x9 -0.002374 0.999370 -4.717265 -0.676115 -0.006132 0.673234 4.276593 TE 3.677760 3.987019 0.000000 0.000000 0.000000 8.000000 8.000000 T 0.430000 0.495078 0.000000 0.000000 0.000000 1.000000 1.000000 Y -0.554922 6.972513 -27.887161 -5.485575 -1.094181 4.055324 29.675579 In\u00a0[69]: Copied! <pre>import plotly.express as px\nfrom IPython.display import Image\n\npx.box(df, x='Y', color='T', width=1200, height=400, title='The distribution of Y split by T').write_image('diagrams/causal_forest/Y_dist.png')\nImage('diagrams/causal_forest/Y_dist.png')\n</pre> import plotly.express as px from IPython.display import Image  px.box(df, x='Y', color='T', width=1200, height=400, title='The distribution of Y split by T').write_image('diagrams/causal_forest/Y_dist.png') Image('diagrams/causal_forest/Y_dist.png') Out[69]: In\u00a0[10]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom econml.grf import CausalForest\n\nt = df['T'].to_numpy()\nX = df[[f'x{i}' for i in range(10)]].to_numpy()\ny = df['Y'].to_numpy()\nTE = df['TE'].to_numpy()\n\nX_train, X_test, y_train, y_test, t_train, t_test, TE_train, TE_test = train_test_split(X, y, t, TE, stratify=t, random_state=42)\n\ncf = CausalForest(random_state=42)\ncf.fit(X=X_train, y=y_train, T=t_train)\n</pre> from sklearn.model_selection import train_test_split from econml.grf import CausalForest  t = df['T'].to_numpy() X = df[[f'x{i}' for i in range(10)]].to_numpy() y = df['Y'].to_numpy() TE = df['TE'].to_numpy()  X_train, X_test, y_train, y_test, t_train, t_test, TE_train, TE_test = train_test_split(X, y, t, TE, stratify=t, random_state=42)  cf = CausalForest(random_state=42) cf.fit(X=X_train, y=y_train, T=t_train)  Out[10]: <pre>CausalForest(random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CausalForest<pre>CausalForest(random_state=42)</pre> In\u00a0[30]: Copied! <pre>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndf_test = pd.DataFrame(X_test, columns=[f'x{i}' for i in range(10)])\ndf_test['TE_test'] = TE_test\ndf_test['TE_cf'] = cf.predict(X_test)\n\nfig = make_subplots(rows=1, cols=1, x_title='x1', y_title='TE', column_titles=['TE_cf'])\n\nfor idx, col in zip([1], ['TE_cf']):\n    fig.add_trace(go.Scatter(x=df_test['x1'], y=df_test['TE_cf'], mode='markers', name=col, marker=dict(color='#636efa', size=5)), row=1, col=idx)\n    fig.add_trace(go.Scatter(x=df_test['x1'], y=df_test['TE_test'], mode='markers', name='TE_test', marker=dict(color='red', size=3)), row=1, col=idx)\n\nfig.update_layout(width=800, height=500, showlegend=False, title='Blue scatters are predicted TE, red scatters are the ground truth').write_image('diagrams/causal_forest/comparison.png')\nImage('diagrams/causal_forest/comparison.png')\n</pre> import plotly.graph_objects as go from plotly.subplots import make_subplots  df_test = pd.DataFrame(X_test, columns=[f'x{i}' for i in range(10)]) df_test['TE_test'] = TE_test df_test['TE_cf'] = cf.predict(X_test)  fig = make_subplots(rows=1, cols=1, x_title='x1', y_title='TE', column_titles=['TE_cf'])  for idx, col in zip([1], ['TE_cf']):     fig.add_trace(go.Scatter(x=df_test['x1'], y=df_test['TE_cf'], mode='markers', name=col, marker=dict(color='#636efa', size=5)), row=1, col=idx)     fig.add_trace(go.Scatter(x=df_test['x1'], y=df_test['TE_test'], mode='markers', name='TE_test', marker=dict(color='red', size=3)), row=1, col=idx)  fig.update_layout(width=800, height=500, showlegend=False, title='Blue scatters are predicted TE, red scatters are the ground truth').write_image('diagrams/causal_forest/comparison.png') Image('diagrams/causal_forest/comparison.png') Out[30]:"},{"location":"notebooks/causal_forest/#intro-to-causal-forest","title":"Intro to causal forest\u00b6","text":"<p>This notebook aims to explain how causal forest works and some key differences compared to the well-known algorithm random forest.</p> <p>I will demo the causal forest APIs from EconML using a toy dataset.</p>"},{"location":"notebooks/causal_forest/#data-generation","title":"Data generation\u00b6","text":"<p>Some synthetic data (100,000 samples and 10 covariates) are generated by following a data generation process that is also presented in a demo notebook in EconML.</p> <p>Process</p> <ul> <li>$x_i = normal(mean=0, std=1)$</li> <li>$Y\\_x_i = x_i * uniform(low=-3, high=3) + normal(mean=0, std=1)$</li> <li>$TE =  \\begin{cases}   8, &amp; \\text{if } x_1 &gt; 0.1 \\\\   0, &amp; \\text{else} \\end{cases}$</li> <li>$propensity = \\begin{cases}   0.8, &amp; \\text{if } -0.5 &lt; x_2 &lt;0.5 \\\\   0.2, &amp; \\text{else}  \\end{cases}$</li> <li>$T = binomaial(prop=propensity, trials=1)$</li> <li>$Y = TE * T + Y\\_x_i$</li> </ul>"},{"location":"notebooks/causal_forest/#causal-forest","title":"Causal forest\u00b6","text":"<p>Causal forest is a method from generalised random forest, it can be used to estimate heterogeneous treatment effects. It is composed of causal trees, each of which is very similar to the decision tree that we are all familiar with. However, there are several subtle differences, including splitting criteria, evaluation methods, etc. But why can't we use a decision tree to solve uplift modeling problems? The key issue for the decision tree is that it is not designed to estimate treatment effects, and it introduces generalization bias. Besides, the ground truth is never observed, making it challenging to directly use a decision tree to estimate treatment effects.</p> <p>In this section, I will walk through some of the key features of causal tree to illustrate the differences between it and the decision tree.</p>"},{"location":"notebooks/causal_forest/#split-criteria","title":"Split criteria\u00b6","text":""},{"location":"notebooks/causal_forest/#honesty","title":"Honesty\u00b6","text":""},{"location":"notebooks/causal_forest/#way-of-estimation","title":"Way of estimation\u00b6","text":""},{"location":"notebooks/causal_forest/#example","title":"Example\u00b6","text":""},{"location":"notebooks/causal_forest/#learning-materials","title":"Learning materials\u00b6","text":"<ul> <li>Causal Forest by Susan Athey</li> <li>Causa forest intuition</li> <li>Causal tree tutorial page 9</li> </ul>"},{"location":"notebooks/uplift_modelling/","title":"Uplift modelling","text":"In\u00a0[1]: Copied! <pre>from IPython.display import Image, display\nImage('diagrams/uplift_modelling/uplift_modelling.md.1.png')\n</pre> from IPython.display import Image, display Image('diagrams/uplift_modelling/uplift_modelling.md.1.png') Out[1]: In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnum_samples = 10000\nnp.random.seed(42)\nx1 = abs(np.random.normal(loc=3000, scale=1500, size=num_samples))\nx2 = abs(np.random.normal(loc=50, scale=10, size=num_samples))\nx3 = abs(np.random.randn(num_samples) + 3) + 0.01\nt = np.random.randint(low=0, high=2, size=num_samples)\n\na = 0.01\nb = 0.3\nc = 3\nd = 2.68\n\nTE = d * x2**2 / 1000\nY = TE * t + a * x1 + b * x2 + c * x3\n\ndf = pd.DataFrame({'income': x1, 'avg_order': x2, 'yrs': x3, 'promotion': t, 'TE': TE, 'profit': Y})\ndf.describe().transpose().drop(columns='count').applymap(lambda x: round(x, 2))\n</pre> import pandas as pd import numpy as np import plotly.express as px import warnings warnings.filterwarnings('ignore')  num_samples = 10000 np.random.seed(42) x1 = abs(np.random.normal(loc=3000, scale=1500, size=num_samples)) x2 = abs(np.random.normal(loc=50, scale=10, size=num_samples)) x3 = abs(np.random.randn(num_samples) + 3) + 0.01 t = np.random.randint(low=0, high=2, size=num_samples)  a = 0.01 b = 0.3 c = 3 d = 2.68  TE = d * x2**2 / 1000 Y = TE * t + a * x1 + b * x2 + c * x3  df = pd.DataFrame({'income': x1, 'avg_order': x2, 'yrs': x3, 'promotion': t, 'TE': TE, 'profit': Y}) df.describe().transpose().drop(columns='count').applymap(lambda x: round(x, 2)) Out[1]: mean std min 25% 50% 75% max income 3022.98 1451.88 1.67 1992.69 2996.11 4006.62 8889.36 avg_order 50.14 10.01 11.44 43.38 50.16 56.94 94.79 yrs 3.00 0.99 0.02 2.31 3.00 3.67 6.70 promotion 0.49 0.50 0.00 0.00 0.00 1.00 1.00 TE 7.00 2.72 0.35 5.04 6.74 8.69 24.08 profit 57.73 15.93 15.44 46.43 57.45 68.44 128.91 In\u00a0[3]: Copied! <pre>print('distribution of promotion')\ndf['promotion'].value_counts()\n</pre> print('distribution of promotion') df['promotion'].value_counts() <pre>distribution of promotion\n</pre> Out[3]: <pre>promotion\n0    5061\n1    4939\nName: count, dtype: int64</pre> In\u00a0[4]: Copied! <pre>df.head()\n</pre> df.head() Out[4]: income avg_order yrs promotion TE profit 0 3745.071230 43.215053 3.358286 1 5.005009 65.495096 1 2792.603548 46.945005 3.293324 1 5.906274 57.795782 2 3971.532807 44.026189 2.073480 1 5.194658 64.338284 3 5284.544785 51.104180 3.589584 1 6.999188 85.944643 4 2648.769938 61.971785 1.519917 0 10.292546 49.638987 In\u00a0[5]: Copied! <pre>px.box(df, x='profit', color='promotion', width=1200, height=400, title='The distribution of profit split by promotion').write_image('diagrams/uplift_modelling/promotion_dist.png')\nImage('diagrams/uplift_modelling/promotion_dist.png')\n</pre> px.box(df, x='profit', color='promotion', width=1200, height=400, title='The distribution of profit split by promotion').write_image('diagrams/uplift_modelling/promotion_dist.png') Image('diagrams/uplift_modelling/promotion_dist.png') Out[5]: In\u00a0[6]: Copied! <pre>Image('diagrams/uplift_modelling/uplift_modelling.md.2.png')\n</pre> Image('diagrams/uplift_modelling/uplift_modelling.md.2.png') Out[6]: In\u00a0[7]: Copied! <pre>from econml.metalearners import SLearner, TLearner\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nt = df['promotion'].to_numpy()\nX = df[['income', 'avg_order', 'yrs']].to_numpy()\ny = df['profit'].to_numpy()\nTE = df['TE'].to_numpy()\n\nX_train, X_test, y_train, y_test, t_train, t_test, TE_train, TE_test = train_test_split(X, y, t, TE, stratify=t, random_state=42)\n</pre> from econml.metalearners import SLearner, TLearner from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error  t = df['promotion'].to_numpy() X = df[['income', 'avg_order', 'yrs']].to_numpy() y = df['profit'].to_numpy() TE = df['TE'].to_numpy()  X_train, X_test, y_train, y_test, t_train, t_test, TE_train, TE_test = train_test_split(X, y, t, TE, stratify=t, random_state=42) In\u00a0[8]: Copied! <pre>for matrix, name in zip([X_train, X_test, y_train, y_test, t_train, t_test, TE_train, TE_test], ['X_train', 'X_test', 'y_train', 'y_test', 't_train', 't_test', 'TE_train', 'TE_test']):\n    print(name+' shape: ', matrix.shape)\n</pre> for matrix, name in zip([X_train, X_test, y_train, y_test, t_train, t_test, TE_train, TE_test], ['X_train', 'X_test', 'y_train', 'y_test', 't_train', 't_test', 'TE_train', 'TE_test']):     print(name+' shape: ', matrix.shape) <pre>X_train shape:  (7500, 3)\nX_test shape:  (2500, 3)\ny_train shape:  (7500,)\ny_test shape:  (2500,)\nt_train shape:  (7500,)\nt_test shape:  (2500,)\nTE_train shape:  (7500,)\nTE_test shape:  (2500,)\n</pre> In\u00a0[9]: Copied! <pre>df_test = pd.DataFrame(X_test, columns=['income', 'avg_order', 'yrs'])\ndf_test['TE_test'] = TE_test\ndf_test.head()\n</pre> df_test = pd.DataFrame(X_test, columns=['income', 'avg_order', 'yrs']) df_test['TE_test'] = TE_test df_test.head() Out[9]: income avg_order yrs TE_test 0 2256.182742 47.746519 1.542728 6.109677 1 7152.560408 54.992585 3.113768 8.104814 2 2968.132831 65.329782 2.041038 11.438188 3 776.404820 50.436401 3.041080 6.817466 4 5980.913999 52.315411 4.121376 7.334898 In\u00a0[10]: Copied! <pre>slearner = SLearner(overall_model=RandomForestRegressor(random_state=42))\nslearner.fit(Y=y_train, T=t_train, X=X_train)\nTE_slearner = slearner.effect(X_test)\ndf_test['TE_slearner'] = TE_slearner\n</pre> slearner = SLearner(overall_model=RandomForestRegressor(random_state=42)) slearner.fit(Y=y_train, T=t_train, X=X_train) TE_slearner = slearner.effect(X_test) df_test['TE_slearner'] = TE_slearner In\u00a0[11]: Copied! <pre>Image('diagrams/uplift_modelling/uplift_modelling.md.3.png')\n</pre> Image('diagrams/uplift_modelling/uplift_modelling.md.3.png') Out[11]: In\u00a0[12]: Copied! <pre>tlearner = TLearner(models=RandomForestRegressor(random_state=42))\ntlearner.fit(Y=y_train, T=t_train, X=X_train)\nTE_tlearner = tlearner.effect(X_test)\ndf_test['TE_tlearner'] = TE_tlearner\n</pre> tlearner = TLearner(models=RandomForestRegressor(random_state=42)) tlearner.fit(Y=y_train, T=t_train, X=X_train) TE_tlearner = tlearner.effect(X_test) df_test['TE_tlearner'] = TE_tlearner In\u00a0[13]: Copied! <pre>display(Image('diagrams/uplift_modelling/uplift_modelling.md.4.png'))\ndisplay(Image('diagrams/uplift_modelling/uplift_modelling.md.5.png'))\n</pre> display(Image('diagrams/uplift_modelling/uplift_modelling.md.4.png')) display(Image('diagrams/uplift_modelling/uplift_modelling.md.5.png'))  In\u00a0[14]: Copied! <pre>from econml.dml import CausalForestDML\nfrom sklearn.ensemble import RandomForestClassifier\n\ncfdml = CausalForestDML(model_y=RandomForestRegressor(random_state=42),\n                        model_t=RandomForestClassifier(random_state=42),\n                        discrete_treatment=True)\n\ncfdml.fit(Y=y_train, T=t_train, X=X_train)\nTE_cfdml = cfdml.effect(X_test)\ndf_test['TE_cfdml'] = TE_cfdml\n</pre> from econml.dml import CausalForestDML from sklearn.ensemble import RandomForestClassifier  cfdml = CausalForestDML(model_y=RandomForestRegressor(random_state=42),                         model_t=RandomForestClassifier(random_state=42),                         discrete_treatment=True)  cfdml.fit(Y=y_train, T=t_train, X=X_train) TE_cfdml = cfdml.effect(X_test) df_test['TE_cfdml'] = TE_cfdml In\u00a0[15]: Copied! <pre>corr_scores = pd.DataFrame.corr(df_test[['TE_test', 'TE_slearner', 'TE_tlearner', 'TE_cfdml']]).iloc[0, 1:4].to_numpy()\nrmse_scores = [np.sqrt(mean_squared_error(TE_test, df_test[col])) for col in ['TE_slearner', 'TE_tlearner', 'TE_cfdml']]\n\ndf_scores = pd.DataFrame({'correlation with TE_test': corr_scores, 'rmse': rmse_scores})\ndf_scores.index = ['TE_slearner', 'TE_tlearner', 'TE_cfdml']\ndf_scores\n</pre> corr_scores = pd.DataFrame.corr(df_test[['TE_test', 'TE_slearner', 'TE_tlearner', 'TE_cfdml']]).iloc[0, 1:4].to_numpy() rmse_scores = [np.sqrt(mean_squared_error(TE_test, df_test[col])) for col in ['TE_slearner', 'TE_tlearner', 'TE_cfdml']]  df_scores = pd.DataFrame({'correlation with TE_test': corr_scores, 'rmse': rmse_scores}) df_scores.index = ['TE_slearner', 'TE_tlearner', 'TE_cfdml'] df_scores Out[15]: correlation with TE_test rmse TE_slearner 0.964588 0.816356 TE_tlearner 0.957411 0.867265 TE_cfdml 0.995627 0.259265 In\u00a0[16]: Copied! <pre>for col in ['TE_slearner', 'TE_tlearner', 'TE_cfdml']:\n    df_test[f'{col}_diff_perc'] = (df_test[col] - df_test['TE_test'])/df_test['TE_test']\n\npx.box(df_test, x=df_test.columns[-3:], width=1000, height=400).write_image('diagrams/uplift_modelling/diff_perc.png')\nImage('diagrams/uplift_modelling/diff_perc.png')\n</pre> for col in ['TE_slearner', 'TE_tlearner', 'TE_cfdml']:     df_test[f'{col}_diff_perc'] = (df_test[col] - df_test['TE_test'])/df_test['TE_test']  px.box(df_test, x=df_test.columns[-3:], width=1000, height=400).write_image('diagrams/uplift_modelling/diff_perc.png') Image('diagrams/uplift_modelling/diff_perc.png') Out[16]: In\u00a0[17]: Copied! <pre>df_test[df_test.columns[-3:]].describe().transpose().drop(columns='count').applymap(lambda x: round(x,2))\n</pre> df_test[df_test.columns[-3:]].describe().transpose().drop(columns='count').applymap(lambda x: round(x,2)) Out[17]: mean std min 25% 50% 75% max TE_slearner_diff_perc -0.04 0.20 -4.38 -0.09 -0.02 0.04 3.73 TE_tlearner_diff_perc -0.01 0.20 -2.62 -0.08 -0.00 0.07 4.45 TE_cfdml_diff_perc 0.02 0.09 -0.12 -0.01 0.01 0.03 3.20 <p>Since we know the ground truth of TE which only varies with the average amount of previous orders, we can visualise the TE predicted vs average amount as below.</p> In\u00a0[18]: Copied! <pre>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=3, x_title='average amount of previos orders', y_title='TE', column_titles=['TE_slearner', 'TE_tlearner', 'TE_cfdml'])\n\nfor idx, col in zip([1,2,3], ['TE_slearner', 'TE_tlearner', 'TE_cfdml']):\n    fig.add_trace(go.Scatter(x=df_test['avg_order'], y=df_test[col], mode='markers', name=col, marker=dict(color='#636efa', size=5)), row=1, col=idx)\n    fig.add_trace(go.Scatter(x=df_test['avg_order'], y=df_test['TE_test'], mode='markers', name='TE_test', marker=dict(color='red', size=3)), row=1, col=idx)\n\nfig.update_layout(width=1400, height=500, showlegend=False, title='Blue scatters are predicted TE, red scatters are the ground truth').write_image('diagrams/uplift_modelling/comparison.png')\nImage('diagrams/uplift_modelling/comparison.png')\n</pre> import plotly.graph_objects as go from plotly.subplots import make_subplots  fig = make_subplots(rows=1, cols=3, x_title='average amount of previos orders', y_title='TE', column_titles=['TE_slearner', 'TE_tlearner', 'TE_cfdml'])  for idx, col in zip([1,2,3], ['TE_slearner', 'TE_tlearner', 'TE_cfdml']):     fig.add_trace(go.Scatter(x=df_test['avg_order'], y=df_test[col], mode='markers', name=col, marker=dict(color='#636efa', size=5)), row=1, col=idx)     fig.add_trace(go.Scatter(x=df_test['avg_order'], y=df_test['TE_test'], mode='markers', name='TE_test', marker=dict(color='red', size=3)), row=1, col=idx)  fig.update_layout(width=1400, height=500, showlegend=False, title='Blue scatters are predicted TE, red scatters are the ground truth').write_image('diagrams/uplift_modelling/comparison.png') Image('diagrams/uplift_modelling/comparison.png') Out[18]: <p>It's clear that CausalForestDML can accurately estimate heterogeneous treatment effect but how about the scenario where we are only interested in the average treatment effect between treatment and control group.</p> <p>Therefore, by taking the average of the predicted TE, we find that t-learner and CausalForestDML have similar performance (6.923-TE_tlearner, 6.996-TE_cfdml, 6.943-TE_test).</p> In\u00a0[19]: Copied! <pre>df_test[['TE_test', 'TE_slearner', 'TE_tlearner', 'TE_cfdml']].describe().transpose().drop(columns='count').applymap(lambda x: round(x,3))\n</pre> df_test[['TE_test', 'TE_slearner', 'TE_tlearner', 'TE_cfdml']].describe().transpose().drop(columns='count').applymap(lambda x: round(x,3)) Out[19]: mean std min 25% 50% 75% max TE_test 6.943 2.716 0.351 4.969 6.707 8.658 17.759 TE_slearner 6.785 2.978 -2.509 4.852 6.472 8.481 19.913 TE_tlearner 6.923 2.971 -2.292 4.926 6.542 8.728 21.381 TE_cfdml 6.996 2.706 1.248 4.940 6.705 8.546 17.268"},{"location":"notebooks/uplift_modelling/#intro-to-uplift-modelling","title":"Intro to uplift modelling\u00b6","text":""},{"location":"notebooks/uplift_modelling/#overview","title":"Overview\u00b6","text":"<p>This notebook demonstrates the process of generating some synthetic data for causal inference problems.</p> <p>The diagram below shows how the profit from a customer is related to other factors.</p> <ul> <li>Profit is the outcome we are interested in.</li> <li>Promotion is believed to have a direct impact on the profit. The impact of a promotion also varies based on the average amount of previous orders.</li> <li>Profit is also associated with three factors.<ul> <li>The income of a customer.</li> <li>The average amount of previous orders.</li> <li>The number of years since registration.</li> </ul> </li> </ul> <p>Below is a more formal mathematical representation.</p> <ul> <li>$ y = TE * t + a * x_1 + b * x_2 + c * x_3$</li> <li>$ TE = d * x_2^2$</li> </ul>"},{"location":"notebooks/uplift_modelling/#treatment-effect-estimation","title":"Treatment effect estimation\u00b6","text":"<p>In many scenarios, we want to find out the uplift/increase in profit from a customer that is driven by promotions. It is equivalent to estimating the treatment effect of running a promotion on a customer.</p> <p>Since the treatment effect varies based on another attribute, it means that the treatment effect is heterogeneous and we want to model conditional average treatment effects (CATE). Being able to do that would enable us to identify the groups of customers who respond positively to promotions, which means that we would be able to run more sophisticated campaigns to maximise the return.</p> <p>The more professional term describing the process above is uplift modelling. There are several algorithms we can use. I will start with talking about meta learner.</p>"},{"location":"notebooks/uplift_modelling/#meta-learner","title":"Meta learner\u00b6","text":"<p>Meta learner is a technique that integrates information from one or multiple machine learning models to generate combined estimate of causal effects. It includes S-learner, T-learner, X-learner. We will be utilising the implementation of EconML.</p>"},{"location":"notebooks/uplift_modelling/#s-learner","title":"S-learner\u200b\u00b6","text":"<ul> <li>Train only one model M() using both treated and control samples.</li> <li>Takes both the treatment T and features X to predict the outcome.\u200b</li> <li>Scoring: the treatment effect is M(X|T=1) - M(X|T=0)</li> </ul>"},{"location":"notebooks/uplift_modelling/#t-learner","title":"T-learner\u00b6","text":"<ul> <li>Handle regularisation bias issue from S-learner\u200b</li> <li>Train two models F_1() and F_0(). F_1 uses treated samples, F_0 uses control samples.\u200b</li> <li>Both model take only features X to predict the outcome Y.\u200b</li> <li>Scoring: the treatment effect is F_1(X) - F_0(T=0)\u200b</li> </ul>"},{"location":"notebooks/uplift_modelling/#dml","title":"DML\u00b6","text":"<p>The next algorithm is DML, a two-stage framework to estimate causal parameters, offers great flexibility to choose first and second stage models.\u200b It is good at handling regularisation bias and confounding bias\u200b.</p> <ul> <li>Stage 1: Partialling-out phase (remove the effects from confounders in treatment and outcome)</li> <li>Stage 2: Heterogeneity modelling phase (regress y_res using t_res and features)\u200b</li> </ul> <p>In this exercise, I will be using one of the variants called CausalForsetDML implemented in EconML.</p>"},{"location":"notebooks/uplift_modelling/#comparison","title":"Comparison\u00b6","text":"<p>In this section, the prediction results are assessed via looking at various metrics, such as correlation score, root mean square error, etc.</p> <p>The correlation and rmse table below shows the TE predicted by causalForestDML is a lot closer to the ground truth. The performance between s-learner and t-learner is not quite noticeable.</p>"},{"location":"notebooks/diagrams/causal_forest/causal_forest/","title":"Causal forest","text":"<pre><code>flowchart\nsubgraph covariates\nx0[x_0]\nx1[x_1]\nx2[x_2]\nxi[x_i]\nend\nx0--a--&gt;y\nx1--b--&gt;y\nx2--c--&gt;y\nxi--n--&gt;y\ny[Y]\nt[T]\nt-- TE, varies with x_1 --&gt;y\nx2--&gt;t\n</code></pre>"},{"location":"notebooks/diagrams/uplift_modelling/uplift_modelling/","title":"Uplift modelling","text":"<pre><code>%%{init: {'theme':'default'}}%%\n\nflowchart\nsubgraph  \ndirection BT\nx1((x1 \\n income))\nx2((x2 \\n avg \\n order))\nx3((x3 \\n yrs since \\n registration))\ny((y \\n profit))\nt((t \\n promotion))\nx1 --a--&gt; y\nt --TE--&gt; y\nx2 --b--&gt; y\nx3 --c--&gt; y\nend\n</code></pre> <pre><code>%%{init: {'theme':'default'}}%%\nflowchart\nsubgraph S_learners\ndirection LR\nsubgraph training\ndirection TB\nsubgraph all_samples\ndirection TB\nX\nt\ny\nend\ninput_train[input]\ninput_train--&gt;m0\ntarget_train[target]\nX--&gt;input_train\nt--&gt;input_train\ny--&gt;target_train\ntarget_train--&gt;m0\nm0[(model M)]\nend\n\nsubgraph predicting\ndirection TB\ninput[X]\nmodel[(model M)]\nt0[X, t=0]\nt1[X, t=1]\nt0--&gt;model\nt1--&gt;model\ninput--&gt;t0\ninput--&gt;t1\ny0[y0]\ny1[y1]\nmodel --predicts--&gt;y0\nmodel --predicts--&gt;y1\ncate[treatment effect/uplift = y1 - y0]\n\ny0--&gt;cate\ny1--&gt;cate\nend\ntraining --&gt; predicting\nend\n</code></pre> <pre><code>%%{init: {'theme':'default'}}%%\nflowchart\nsubgraph T_learners\nsubgraph training\nsubgraph all_samples\nX\nt\ny\nend\nsubgraph control_samples,t=0\nx_control[X]\ny_control[y]\nend\n\nsubgraph treated_samples,t=1\nx_treatment[X]\ny_treatment[y]\n\nend\nall_samples --&gt; control_samples,t=0\nall_samples --&gt; treated_samples,t=1\nm0[(control model M_0)]\nm1[(treatment model M_1)]\ninput_train_control--&gt;m0\ninput_train_treatment--&gt;m1\ntarget_train_control--&gt;m0\ntarget_train_treatment--&gt;m1\ninput_train_control[input]\ninput_train_treatment[input]\ntarget_train_control[target]\ntarget_train_treatment[target]\n\nx_control--&gt;input_train_control\ny_control--&gt;target_train_control\nx_treatment--&gt;input_train_treatment\ny_treatment--&gt;target_train_treatment\n\nend\n\nsubgraph predicting\ninput[X]\nm0_pred[(control model M_0)]\nm1_pred[(treatment model M_1)]\ninput --&gt; m0_pred\ninput --&gt; m1_pred\ny0[y0]\ny1[y1]\nm0_pred --predicts--&gt;y0\nm1_pred --predicts--&gt;y1\ncate[treatment effect/uplift = y1 - y0]\n\ny0--&gt;cate\ny1--&gt;cate\nend\ntraining --&gt; predicting\nend\n</code></pre> <pre><code>%%{init: {'theme':'default'}}%%\nflowchart\nsubgraph stage_1\ndirection TB\nsubgraph  \nmt[(treatment model, M_t)]\nmy[(outcome model, M_y)]\nt_pred[treatment predicted, t_pred]\ny_pred[outcome predicted, y_pred]\nt_res[treatment residual, t_res]\ny_res[outcome residual, y_res]\nmt--&gt;t_pred\nmy--&gt;y_pred\nt_pred--t_true - t_pred--&gt;t_res\ny_pred--y_true - y_pred--&gt;y_res\ninput[input]\ntarget_y[target]\ntarget_t[target]\ntarget_y--&gt;my\ntarget_t--&gt;mt\ninput--&gt;mt\ninput--&gt;my\nend\nfeature[feature, x]\ntreatment --&gt; target_t\nfeature --&gt; input\ntreatment[treatment, t]\noutcome[outcome, y]\noutcome --&gt; target_y\nend\n</code></pre> <pre><code>%%{init: {'theme':'default'}}%%\nflowchart\nsubgraph  \ndirection TB\nsubgraph stage_2\ndirection TB\nt_res\ny_res[y_res]\ntarget\ny_res--&gt;target\nt_res[t_res]\nt_res--&gt;input\ninput\nfeature[feature, x]\nfeature--&gt;input\nmodel[(CATE model, M_cate)]\ninput--&gt;model\ntarget--&gt;model\ncate[treatment effect/uplift for each sample]\nmodel-- output --&gt;cate\nend\nend\n</code></pre>"}]}